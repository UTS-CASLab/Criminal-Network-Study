{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import array\n",
    "import operator\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable name is not matching graph name, just for save some time\n",
    "G_Meeting_processed = nx.read_weighted_edgelist(\"Data/Combined_Edge_List.csv\", create_using=nx.Graph, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retaining only the largest connected component in G_Meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(G_Meeting_processed))\n",
    "large_components = [c for c in components if len(c) >= 5]\n",
    "G_Meeting_processed = G_Meeting_processed.subgraph(large_components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(G_Meeting_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodePos = nx.spring_layout(G_Meeting_processed, seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node removal and LCC drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collective_influence_centrality(Graph, weight=None):\n",
    "    \"\"\"\n",
    "    Compute Collective Influence (CI) Centrality per each node (up to distance d=2).\n",
    "\n",
    "    :param Graph: (Graph obj) Input Graph.\n",
    "    :param weight : (string) None or string, optional (default=None)\n",
    "      If None, all edge weights are considered equal.\n",
    "      Otherwise holds the name of the edge attribute used as weight.\n",
    "    :return: (dict) Dictionary of nodes with their respective CI Centrality values.\n",
    "    \"\"\"\n",
    "    colinf = dict()\n",
    "    for node in Graph:\n",
    "        summatory = 0\n",
    "        for iter_node in Graph.neighbors(node):\n",
    "            if weight is None:\n",
    "                summatory += Graph.degree(iter_node) - 1\n",
    "            else:\n",
    "                summatory += Graph.degree(iter_node, weight='weight') - 1\n",
    "        if weight is None:\n",
    "            colinf[node] = (Graph.degree(node) - 1) * summatory\n",
    "        else:\n",
    "            colinf[node] = (Graph.degree(node, weight='weight') - 1) * summatory\n",
    "    return colinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcc_size(Graph):\n",
    "    \"\"\"\n",
    "    Compute Largest Connected Component (LCC) in a Graph.\n",
    "    :param Graph: (Graph obj) Input Graph.\n",
    "    :return: (int) Size of the LCC.\n",
    "    \"\"\"\n",
    "    lcc_size = 0\n",
    "    for c in nx.connected_components(Graph):\n",
    "        if len(c) > lcc_size:\n",
    "            lcc_size = len(c)\n",
    "    return lcc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_centrality_nodes(Graph, centrality_function, tiebreaker_function=None, top_n=5, weight=None):\n",
    "    top_nodes = []  # Initialize the list to store the top-n nodes\n",
    "\n",
    "    # Calculate the primary centrality for each node in the graph\n",
    "    if weight is None:\n",
    "        primary_centrality = centrality_function(Graph)\n",
    "        if tiebreaker_function is not None:\n",
    "            secondary_centrality = tiebreaker_function(Graph)\n",
    "    else:\n",
    "        primary_centrality = centrality_function(Graph, weight='weight')\n",
    "        if tiebreaker_function is not None:\n",
    "            secondary_centrality = tiebreaker_function(Graph, weight='weight')\n",
    "\n",
    "    # Sort the nodes based on their primary centrality values in descending order\n",
    "    sorted_primary_centrality = sorted(primary_centrality.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # print(sorted_primary_centrality)\n",
    "    # print()\n",
    "    final_sorted_nodes = []  # List to store the nodes in their final order\n",
    "\n",
    "    if tiebreaker_function is not None:\n",
    "        # Sort the nodes based on their secondary centrality values\n",
    "        sorted_secondary_centrality = sorted(secondary_centrality.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        # print(sorted_secondary_centrality)\n",
    "        secondary_centrality_dict = {key: value for key, value in sorted_secondary_centrality}\n",
    "\n",
    "        # Group the nodes with identical primary centrality values\n",
    "        groups = [dict(g) for k, g in groupby(sorted_primary_centrality, key=lambda x: x[1])]\n",
    "\n",
    "        for group in groups:\n",
    "            if len(group) == 1:  # If there's only one node in a group, add it to final_sorted_nodes\n",
    "                final_sorted_nodes.extend(list(group.keys()))\n",
    "            else:  # If there are multiple nodes, sort them based on their secondary centrality and add them to final_sorted_nodes\n",
    "                group = [k for k in sorted(group, key=lambda k: list(secondary_centrality_dict.keys()).index(k))]\n",
    "                final_sorted_nodes.extend(group)\n",
    "    else:\n",
    "        final_sorted_nodes = [t[0] for t in sorted_primary_centrality]\n",
    "\n",
    "    # Add the first top_n nodes from the final_sorted_nodes list to top_nodes\n",
    "    for i in range(0, top_n):\n",
    "        top_nodes.append(final_sorted_nodes[i])\n",
    "\n",
    "    return top_nodes  # Return the list containing the top-n nodes based on the primary centrality function (and tiebreaker_function, if provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using APs \n",
    "def sorted_aps(G, tiebreaker_function=None):\n",
    "    # Get a list of articulation points\n",
    "    aps = list(nx.articulation_points(G))\n",
    "    \n",
    "    # Create a list to store the articulation points and their LCC sizes after removal\n",
    "    aps_lcc = []\n",
    "\n",
    "    # If a tie-breaker function is provided, calculate the tie-breaker metric for each node\n",
    "    if tiebreaker_function is not None:\n",
    "        tiebreaker_metric = tiebreaker_function(G)\n",
    "\n",
    "    for ap in aps:\n",
    "        # Create a copy of the graph and remove the current articulation point\n",
    "        subgraph_nodes = [node for node in G.nodes if node != ap]\n",
    "        G_subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "        # Get the size of the largest connected component after the removal\n",
    "        lcc_size_current = lcc_size(G_subgraph)\n",
    "\n",
    "        # If a tie-breaker function is provided, store the articulation point, its LCC size and its tie-breaker metric\n",
    "        # Otherwise, only store the articulation point and its LCC size\n",
    "        if tiebreaker_function is not None:\n",
    "            aps_lcc.append((ap, lcc_size_current, tiebreaker_metric[ap]))\n",
    "        else:\n",
    "            aps_lcc.append((ap, lcc_size_current))\n",
    "\n",
    "    # If a tie-breaker function is provided, sort by LCC size in ascending order and then by the tie-breaker metric in descending order for ties\n",
    "    # Otherwise, just sort by LCC size in ascending order\n",
    "    if tiebreaker_function is not None:\n",
    "        sorted_aps_lcc = sorted(aps_lcc, key=lambda x: (x[1], -x[2]))\n",
    "    else:\n",
    "        sorted_aps_lcc = sorted(aps_lcc, key=itemgetter(1))\n",
    "\n",
    "    return sorted_aps_lcc\n",
    "\n",
    "# when top_n is bigger than the number of aps, using tiebreadker_function to complete the top_n nodes\n",
    "def top_aps(G, tiebreaker_function=None, top_n=1):\n",
    "    # Get the ranked articulation points\n",
    "    sorted_aps_lcc = sorted_aps(G, tiebreaker_function=tiebreaker_function)\n",
    "\n",
    "    # Collect the top 'n' articulation points\n",
    "    top_nodes = [ap for ap in (t[0] for t in sorted_aps_lcc)]\n",
    "\n",
    "    # If top_n is bigger than the number of articulation points and no tiebreaker function is provided, raise an error\n",
    "    if top_n > len(top_nodes) and tiebreaker_function is None:\n",
    "        raise ValueError(f\"The number of top nodes requested ({top_n}) is greater than the number of articulation points ({len(top_nodes)}), and no tiebreaker function was provided.\")\n",
    "\n",
    "    # If top_n is bigger than the number of articulation points, fill the top_nodes using tiebreaker_function\n",
    "    if top_n > len(top_nodes):\n",
    "        # Calculate the tiebreaker metric for all nodes in the graph\n",
    "        all_nodes_metric = tiebreaker_function(G)\n",
    "\n",
    "        # Remove the articulation points from the all_nodes_metric dict\n",
    "        for ap in top_nodes:\n",
    "            all_nodes_metric.pop(ap, None)\n",
    "\n",
    "        # Sort the remaining nodes by the tiebreaker metric in descending order\n",
    "        sorted_remaining_nodes = sorted(all_nodes_metric.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Append nodes to top_nodes until its length is top_n\n",
    "        for node, _ in sorted_remaining_nodes:\n",
    "            if len(top_nodes) >= top_n:\n",
    "                break\n",
    "            top_nodes.append(node)\n",
    "\n",
    "    return top_nodes[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CoreHD\n",
    "def core_degrees(G):\n",
    "    # Find the 2-core of the network\n",
    "    core = nx.k_core(G, k=2)\n",
    "\n",
    "    # Get the degree of every node within this 2-core\n",
    "    degrees = dict(core.degree())\n",
    "\n",
    "    return degrees\n",
    "\n",
    "def sorted_core_nodes(G, tiebreaker_function=None):\n",
    "    # Get the degrees of the nodes in the 2-core\n",
    "    core_degrees_dict = core_degrees(G)\n",
    "    \n",
    "    # If a tie-breaker function is provided, calculate the tie-breaker metric for each node\n",
    "    if tiebreaker_function is not None:\n",
    "        tiebreaker_metric = tiebreaker_function(G)\n",
    "    \n",
    "    # Create a list to store the nodes and their degrees\n",
    "    core_nodes = []\n",
    "    \n",
    "    for node, degree in core_degrees_dict.items():\n",
    "        # If a tie-breaker function is provided, store the node, its degree, and its tie-breaker metric\n",
    "        # Otherwise, only store the node and its degree\n",
    "        if tiebreaker_function is not None:\n",
    "            core_nodes.append((node, degree, tiebreaker_metric[node]))\n",
    "        else:\n",
    "            core_nodes.append((node, degree))\n",
    "    \n",
    "    # If a tie-breaker function is provided, sort by degree in descending order and then by the tie-breaker metric in descending order for ties\n",
    "    # Otherwise, just sort by degree in descending order\n",
    "    if tiebreaker_function is not None:\n",
    "        sorted_core_nodes = sorted(core_nodes, key=lambda x: (-x[1], -x[2]))\n",
    "    else:\n",
    "        sorted_core_nodes = sorted(core_nodes, key=lambda x: -x[1])\n",
    "\n",
    "    return sorted_core_nodes\n",
    "\n",
    "def top_core_nodes(G, tiebreaker_function=None, top_n=1):\n",
    "    # Get the ranked core nodes\n",
    "    sorted_nodes = sorted_core_nodes(G, tiebreaker_function=tiebreaker_function)\n",
    "\n",
    "    # Collect the top 'n' core nodes\n",
    "    top_nodes = [node for node in (t[0] for t in sorted_nodes)]\n",
    "\n",
    "    # If top_n is bigger than the number of core nodes and no tiebreaker function is provided, raise an error\n",
    "    if top_n > len(top_nodes) and tiebreaker_function is None:\n",
    "        raise ValueError(f\"The number of top nodes requested ({top_n}) is greater than the number of nodes in the 2-core ({len(top_nodes)}), and no tiebreaker function was provided.\")\n",
    "\n",
    "    # If top_n is bigger than the number of core nodes, fill the top_nodes using tiebreaker_function\n",
    "    if top_n > len(top_nodes):\n",
    "        # print(\"No Core existed, using DEG!\")\n",
    "        # Calculate the tiebreaker metric for all nodes in the graph\n",
    "        all_nodes_metric = tiebreaker_function(G)\n",
    "\n",
    "        # Remove the core nodes from the all_nodes_metric dict\n",
    "        for node in top_nodes:\n",
    "            all_nodes_metric.pop(node, None)\n",
    "\n",
    "        # Sort the remaining nodes by the tiebreaker metric in descending order\n",
    "        sorted_remaining_nodes = sorted(all_nodes_metric.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Append nodes to top_nodes until its length is top_n\n",
    "        for node, _ in sorted_remaining_nodes:\n",
    "            if len(top_nodes) >= top_n:\n",
    "                break\n",
    "            top_nodes.append(node)\n",
    "\n",
    "    return top_nodes[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality(Graph, weight=None):\n",
    "    \"\"\"Compute the degree centrality for nodes. From NetworkX, but adapted for weighted graphs.\n",
    "    ----------\n",
    "    Graph : graph\n",
    "      A networkx graph\n",
    "    weight : None or string, optional (default=None)\n",
    "      If None, all edge weights are considered equal.\n",
    "      Otherwise holds the name of the edge attribute used as weight.\n",
    "    Returns\n",
    "    -------\n",
    "    nodes : dictionary\n",
    "       Dictionary of nodes with degree centrality as the value.\n",
    "    \"\"\"\n",
    "    if Graph.number_of_edges() == 0:\n",
    "            return {node: 1 for node in Graph}\n",
    "        \n",
    "    if weight is None:\n",
    "        centrality = {node: d / (len(Graph) - 1.0) for node, d in Graph.degree()}\n",
    "    else:\n",
    "        degrees_dict = dict(nx.degree(Graph, weight='weight'))\n",
    "        centrality = {node: d / max(degrees_dict.values()) for node, d in degrees_dict.items()}\n",
    "    return centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GRD\n",
    "def GRD(G, n=1):\n",
    "    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    if not connected_components:\n",
    "        return None\n",
    "\n",
    "    top_n_components_nodes = [node for component in connected_components[:n] for node in component]\n",
    "    min_lcc_size = len(connected_components[0])\n",
    "    nodes_to_remove = None\n",
    "\n",
    "    for node_tuple in combinations(top_n_components_nodes, n):\n",
    "        subgraph_nodes = [n for n in top_n_components_nodes if n not in node_tuple]\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "        lcc_size_current = lcc_size(subgraph)\n",
    "        if lcc_size_current <= min_lcc_size:\n",
    "            min_lcc_size = lcc_size_current\n",
    "            nodes_to_remove = node_tuple\n",
    "                \n",
    "    \n",
    "    return list(nodes_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SF-GRD\n",
    "def SF_GRD(G, n=1):\n",
    "    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    if not connected_components:\n",
    "        return None\n",
    "\n",
    "    # Create a subgraph with nodes from the top n components\n",
    "    top_n_components_nodes = [node for component in connected_components[:n] for node in component]\n",
    "    G_subgraph = G.subgraph(top_n_components_nodes)\n",
    "    \n",
    "    # Calculate properties for nodes in the subgraph\n",
    "    betweenness_centrality = nx.betweenness_centrality(G_subgraph)\n",
    "    degree_centrality = nx.degree_centrality(G_subgraph)\n",
    "    articulation_points = list(nx.articulation_points(G_subgraph))\n",
    "\n",
    "    # Select top 5 nodes by betweenness and degree centrality and all articulation points\n",
    "    top_betweenness_nodes = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)[:5]\n",
    "    top_degree_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:5]\n",
    "    top_aps = sorted(articulation_points, key=lambda x: G_subgraph.degree(x), reverse=True)[:5]\n",
    "\n",
    "    search_nodes = set(top_betweenness_nodes + top_degree_nodes + top_aps)\n",
    "\n",
    "    min_lcc_size = lcc_size(G_subgraph)\n",
    "    nodes_to_remove = None\n",
    "\n",
    "    for node_tuple in combinations(search_nodes, n):\n",
    "        subgraph_nodes = [node for node in top_n_components_nodes if node not in node_tuple]\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "        lcc_size_current = lcc_size(subgraph)\n",
    "        if lcc_size_current <= min_lcc_size:\n",
    "            min_lcc_size = lcc_size_current\n",
    "            nodes_to_remove = list(node_tuple)\n",
    "\n",
    "    return list(nodes_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nodes_to_remove(graph, main_centr, second_centr=None, block_size=1, weight=None):\n",
    "    if main_centr == 'APs':\n",
    "        return top_aps(graph, tiebreaker_function=second_centr, top_n=block_size)\n",
    "    elif main_centr == 'CoreHD':\n",
    "        return top_core_nodes(graph, tiebreaker_function=second_centr, top_n=block_size)\n",
    "    elif main_centr == 'GRD':\n",
    "        return GRD(graph, n=block_size)\n",
    "    elif main_centr == 'SF-GRD':\n",
    "        return SF_GRD(graph, n=block_size)\n",
    "    else:\n",
    "        return max_centrality_nodes(graph, centrality_function=main_centr, tiebreaker_function=second_centr, top_n=block_size, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including Direct Optimisation, AP, and Centrality measure\n",
    "def disruption(Graph, main_centr, second_centr=None, block_size=1, within_LCC=False, weight=None, percentage=0.2):\n",
    "    Graph = Graph.copy()\n",
    "    N = Graph.number_of_nodes()  # Total number of nodes\n",
    "    target_nodes_to_remove = int(N * percentage)\n",
    "    \n",
    "    graph_snapshots = {'Graphs': [], 'Nodes': []}\n",
    "    lcc_sizes = dict()  \n",
    "    kiter = 0\n",
    "    lcc_sizes[kiter] = lcc_size(Graph)\n",
    "    nodes_removed = 0\n",
    "    while nodes_removed < target_nodes_to_remove and Graph.number_of_nodes() >= block_size:\n",
    "        \n",
    "        if within_LCC:\n",
    "            largest_cc = max(nx.connected_components(Graph), key=len)\n",
    "    \n",
    "            if len(largest_cc) < block_size:\n",
    "                subgraph = Graph\n",
    "            else:    \n",
    "                subgraph = Graph.subgraph(largest_cc)\n",
    "        else:\n",
    "            subgraph = Graph\n",
    "\n",
    "        toremove = select_nodes_to_remove(subgraph, main_centr, second_centr, block_size, weight)\n",
    "        \n",
    "#         if not toremove:\n",
    "#             break\n",
    "        graph_snapshots['Graphs'].append(Graph.copy())\n",
    "        graph_snapshots['Nodes'].append(toremove)\n",
    "        \n",
    "        Graph.remove_nodes_from(toremove)\n",
    "        nodes_removed += len(toremove)\n",
    "            \n",
    "        kiter += block_size\n",
    "        current_lcc_size = lcc_size(Graph)\n",
    "        lcc_sizes[kiter] = current_lcc_size\n",
    "    \n",
    "    R = (sum(lcc_sizes.values()) - lcc_sizes[0]) / (N * (len(lcc_sizes) - 1))\n",
    "\n",
    "    return R, lcc_sizes, graph_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the properties of nodes found by direct optimisation method\n",
    "def GRD_analysis(G, n=1):\n",
    "    # 1. Calculate properties for all nodes\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    articulation_points = list(nx.articulation_points(G))\n",
    "\n",
    "    # Sort nodes by properties to get rankings\n",
    "    sorted_by_betweenness = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
    "    sorted_by_degree = sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
    "\n",
    "    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    if not connected_components:\n",
    "        return None, None\n",
    "\n",
    "    top_n_components_nodes = [node for component in connected_components[:n] for node in component]\n",
    "    min_lcc_size = len(connected_components[0])\n",
    "    nodes_to_remove = None\n",
    "    node_properties = None\n",
    "\n",
    "    if n == 1:\n",
    "        for node in top_n_components_nodes:\n",
    "            subgraph_nodes = [n for n in top_n_components_nodes if n != node]\n",
    "            subgraph = G.subgraph(subgraph_nodes)\n",
    "            lcc_size_current = lcc_size(subgraph)\n",
    "            if lcc_size_current < min_lcc_size:\n",
    "                min_lcc_size = lcc_size_current\n",
    "                nodes_to_remove = [node]\n",
    "                if nodes_to_remove is not None:  # Add the condition here\n",
    "                    node_properties = {\n",
    "                        'betweenness_rank': sorted_by_betweenness.index(node) + 1,\n",
    "                        'degree_rank': sorted_by_degree.index(node) + 1,\n",
    "                        'is_ap': node in articulation_points,\n",
    "                    }\n",
    "    else:\n",
    "        for node_tuple in combinations(top_n_components_nodes, n):\n",
    "            subgraph_nodes = [n for n in top_n_components_nodes if n not in node_tuple]\n",
    "            subgraph = G.subgraph(subgraph_nodes)\n",
    "            lcc_size_current = lcc_size(subgraph)\n",
    "            if lcc_size_current < min_lcc_size:\n",
    "                min_lcc_size = lcc_size_current\n",
    "                nodes_to_remove = list(node_tuple)\n",
    "                if nodes_to_remove is not None:  # Add the condition here\n",
    "                    node_properties = [{\n",
    "                        'node': node,  \n",
    "                        'betweenness_rank': sorted_by_betweenness.index(node) + 1,\n",
    "                        'degree_rank': sorted_by_degree.index(node) + 1,\n",
    "                        'is_ap': node in articulation_points,\n",
    "                    } for node in node_tuple]\n",
    "                \n",
    "    return nodes_to_remove, node_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parameter percentage (remove a specific number of nodes according to the given percentage)\n",
    "def disruption_GRD_analysis(Graph, main_centr, second_centr=None, block_size=1, within_LCC=False, weight=None, percentage=0.2):\n",
    "    Graph = Graph.copy()\n",
    "    N = Graph.number_of_nodes()  # Total number of nodes\n",
    "    target_nodes_to_remove = int(N * percentage)\n",
    "\n",
    "    graph_snapshots = {'Graphs': [], 'Nodes': []}\n",
    "    directly_removed_nodes_properties = {}  # Separate dict to store properties of directly removed nodes\n",
    "    lcc_sizes = dict()\n",
    "    kiter = 0\n",
    "    lcc_sizes[kiter] = lcc_size(Graph)\n",
    "\n",
    "    nodes_removed = 0\n",
    "    while nodes_removed < target_nodes_to_remove and Graph.number_of_nodes() >= block_size:\n",
    "        if within_LCC:\n",
    "            largest_cc = max(nx.connected_components(Graph), key=len)\n",
    "            if len(largest_cc) < block_size:\n",
    "                break\n",
    "            subgraph = Graph.subgraph(largest_cc).copy()\n",
    "        else:\n",
    "            subgraph = Graph\n",
    "\n",
    "        properties = None\n",
    "        if main_centr == 'GRD-Analysis':\n",
    "            toremove, properties = GRD_analysis(subgraph, n=block_size)\n",
    "            if properties is not None:\n",
    "                key = tuple(toremove) if isinstance(toremove, list) else toremove\n",
    "                directly_removed_nodes_properties[key] = properties\n",
    "\n",
    "        graph_snapshots['Graphs'].append(Graph.copy())\n",
    "        graph_snapshots['Nodes'].append(toremove)\n",
    "\n",
    "        if not toremove:\n",
    "            break\n",
    "\n",
    "        Graph.remove_nodes_from(toremove)\n",
    "        nodes_removed += len(toremove)\n",
    "        \n",
    "        kiter += block_size\n",
    "        current_lcc_size = lcc_size(Graph)\n",
    "        lcc_sizes[kiter] = current_lcc_size\n",
    "\n",
    "    R = (sum(lcc_sizes.values()) - lcc_sizes[0]) / (N * (len(lcc_sizes) - 1))\n",
    "\n",
    "    return R, lcc_sizes, graph_snapshots, directly_removed_nodes_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, lcc_sizes , graph_snapshots, directly_removed_nodes_properties = disruption_GRD_analysis(G_Meeting_processed, main_centr='GRD-Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(directly_removed_nodes_properties, orient='index').reset_index()\n",
    "df.columns = ['node', 'betweenness_rank', 'degree_rank', 'is_ap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2, lcc_sizes_2, graph_snapshots_2, directly_removed_nodes_properties_2 = disruption_GRD_analysis(G_Meeting_processed, main_centr=\"GRD-Analysis\", block_size=2, percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in directly_removed_nodes_properties_2.values() for item in sublist]\n",
    "df2 = pd.DataFrame(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_3, lcc_sizes_3, graph_snapshots_3, directly_removed_nodes_properties_3 = disruption_GRD_analysis(G_Meeting_processed, main_centr=\"GRD-Analysis\", block_size=3, percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in directly_removed_nodes_properties_3.values() for item in sublist]\n",
    "df3 = pd.DataFrame(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_4, lcc_sizes_4, graph_snapshots_4, directly_removed_nodes_properties_4 = disruption_GRD_analysis(G_Meeting_processed, main_centr=\"GRD-Analysis\", block_size=4, percentage=0.2)\n",
    "\n",
    "flat_list = [item for sublist in directly_removed_nodes_properties_4.values() for item in sublist]\n",
    "df4 = pd.DataFrame(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20, 11))\n",
    "\n",
    "# Adjust to have 4 rows and 3 columns\n",
    "gs = gridspec.GridSpec(4, 3, width_ratios=[3.5, 3.6, 0.9]) \n",
    "\n",
    "# Common font size\n",
    "font_size = 16\n",
    "    \n",
    "def plot_data(ax, data, xlabel=None, ylabel=None, sort_ascending=True, hatch_style=None):\n",
    "    data.value_counts().sort_index(ascending=sort_ascending).plot(kind='bar', facecolor='none', edgecolor='black', linewidth=2, ax=ax, hatch=hatch_style)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel, fontsize=font_size + 1)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=font_size + 1)\n",
    "    total = len(data)\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        ax.annotate(percentage, (p.get_x(), p.get_height() + 0.15), color='black', fontsize=font_size - 3)\n",
    "    ax.tick_params(axis='x', labelsize=font_size, rotation=0)\n",
    "    ax.tick_params(axis='y', labelsize=font_size)\n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Formatter for y-axis\n",
    "y_format = StrMethodFormatter('{x:.0f}')\n",
    "\n",
    "# First row (no x-labels)\n",
    "ax0 = plt.subplot(gs[0])\n",
    "plot_data(ax0, df['betweenness_rank'], ylabel='Frequency (b=1)', hatch_style=' ')\n",
    "ax0.yaxis.set_major_formatter(y_format)\n",
    "\n",
    "plot_data(plt.subplot(gs[1]), df['degree_rank'], hatch_style='//')\n",
    "plot_data(plt.subplot(gs[2]), df['is_ap'], sort_ascending=False, hatch_style='..')\n",
    "\n",
    "# Second row (no x-labels)\n",
    "plot_data(plt.subplot(gs[3]), df2['betweenness_rank'], ylabel='Frequency (b=2)', hatch_style=' ')\n",
    "plot_data(plt.subplot(gs[4]), df2['degree_rank'], hatch_style='//')\n",
    "plot_data(plt.subplot(gs[5]), df2['is_ap'],  sort_ascending=False, hatch_style='..')\n",
    "\n",
    "# Third row (no x-labels)\n",
    "plot_data(plt.subplot(gs[6]), df3['betweenness_rank'], ylabel='Frequency (b=3)', hatch_style=' ')\n",
    "plot_data(plt.subplot(gs[7]), df3['degree_rank'], hatch_style='//')\n",
    "plot_data(plt.subplot(gs[8]), df3['is_ap'], sort_ascending=False, hatch_style='..')\n",
    "\n",
    "# Fourth row (with x-labels)\n",
    "plot_data(plt.subplot(gs[9]), df4['betweenness_rank'], 'Betweenness Rank', 'Frequency (b=4)', hatch_style=' ')\n",
    "plot_data(plt.subplot(gs[10]), df4['degree_rank'], 'Degree Rank', hatch_style='//')\n",
    "plot_data(plt.subplot(gs[11]), df4['is_ap'], 'is_AP', sort_ascending=False, hatch_style='..')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grd_analysis.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time disruption(G_Meeting_processed, main_centr=\"GRD\", block_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time disruption(G_Meeting_processed, main_centr=\"SF-GRD\", block_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time disruption(G_Meeting_processed, main_centr=\"GRD\", block_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time disruption(G_Meeting_processed, main_centr=\"SF-GRD\", block_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centr_measures = {'Betweenness': [nx.betweenness_centrality, None],\n",
    "                  'Betweenness-Degree': [nx.betweenness_centrality, degree_centrality],\n",
    "                  'CI': [collective_influence_centrality, None],\n",
    "                  'Degree': [degree_centrality, None],\n",
    "                  'Degree-Betweenness': [degree_centrality, nx.betweenness_centrality],\n",
    "                  'CoreHD': ['CoreHD', degree_centrality],\n",
    "                  'APs-Degree':[\"APs\", degree_centrality],\n",
    "                  'GRD' : ['GRD', None],\n",
    "                  'SF-GRD' : ['SF-GRD', None]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centrality_disruption_analysis(graph, centrality_measures, include_within_LCC=True, block_size=1, percentage=0.2):\n",
    "    df_lcc_all = pd.DataFrame()\n",
    "    graph_snapshots = {}\n",
    "    R_values = {}  # Dictionary to store R values for each centrality measure\n",
    "\n",
    "    for name, function in centrality_measures.items():\n",
    "        R, lcc_sizes, dict_graphs_nodes = disruption(graph, main_centr=function[0], second_centr=function[1], block_size=block_size, within_LCC=False, weight=None, percentage=percentage)\n",
    "\n",
    "        print(f\"The value of R for {name} is {R}\")\n",
    "        R_values[name] = R  # Store R value in the dictionary\n",
    "\n",
    "        # Store the dictionaries with a key corresponding to the centrality measure name\n",
    "        graph_snapshots[name] = dict_graphs_nodes\n",
    "\n",
    "        df_lcc_all[name] = pd.Series(lcc_sizes)\n",
    "        df_lcc_all.index.name = 'Iteration (' + 'block size: ' + str(block_size) + ')'\n",
    "\n",
    "        if include_within_LCC:\n",
    "            R_within, lcc_sizes_within, dict_graphs_nodes_within = disruption(graph, main_centr=function[0], second_centr=function[1], block_size=block_size, within_LCC=True, weight=None, percentage=percentage)\n",
    "\n",
    "            print(f\"The value of R for {name} within LCC is {R_within}\")\n",
    "            R_values[name + \" within LCC\"] = R_within  # Store R value in the dictionary\n",
    "\n",
    "            # Store the dictionaries with a key corresponding to the centrality measure name and a suffix\n",
    "            graph_snapshots[name + \" within LCC\"] = dict_graphs_nodes_within\n",
    "\n",
    "            df_lcc_all[name + \" within LCC\"] = pd.Series(lcc_sizes_within)\n",
    "\n",
    "    return R_values, df_lcc_all, graph_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_creation(dflcc, typerem, input_name, w):\n",
    "    \"\"\"\n",
    "    Network Disruption Plot.\n",
    "    :param tosave: (string) name path.\n",
    "    :param dflcc: (pandas.core.frame.DataFrame) Largest Connected Component Dataframe.\n",
    "    :param typerem: (string) Type of node removal. It can be 'Sequential' or 'Block'\n",
    "    :param input_name: (string) Name of Input Dataset. It can be 'Meeting' or 'PhoneCalls'\n",
    "    :param w: (string) it can be 'Weighted' or 'Unweighted'\n",
    "    \"\"\"\n",
    "    colnames = list(dflcc.columns)\n",
    "    n_rows = dflcc.shape[0]\n",
    "    \n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "#     plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "#     plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "\n",
    "    #xlabel = colnames[0]\n",
    "    xlabel = dflcc.index\n",
    "    idx = list(range(0, n_rows, 5))\n",
    "    idx = list(range(0, dflcc.index[-1], 5))\n",
    "    plt.grid(True, linestyle=':')\n",
    "    for ylab in colnames[:]:\n",
    "        ax = sns.lineplot(x=xlabel, y=ylab, markers=True, dashes=False, data=dflcc, label=ylab, lw=4, marker=\"o\")\n",
    "    \n",
    "    ax.set_title(input_name, fontsize=24)\n",
    "    ax.set_xticks(idx)\n",
    "    ax.set_xlabel('Number of Nodes Removed', fontsize=20)\n",
    "    ax.set_ylabel('LCC Size', fontsize=20)\n",
    "    ax.yaxis.set_label_coords(0.05, 0.5)  # Adjust the x-coordinate to move the label into the graph\n",
    "    \n",
    "    ax.legend(fontsize=20)  # , prop=legend_properties)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    # Uncomment below for a detailed plot of first 30 iterations, discarding the others.\n",
    "    # ax.set(xlim=(0, 30))\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    fig.set_size_inches((11, 9), forward=False)\n",
    "#     fig.savefig('{0}_{1}_{2}-plos.png'.format(input_name, typerem, w),\n",
    "#                 dpi=300, format='png')\n",
    "    fig.savefig('{0}_{1}_{2}.pdf'.format(input_name, typerem, w))\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "block_size = 1\n",
    "R_values, df_lcc_all, graph_snapshots = centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size, percentage=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2\n",
    "R_values_2, df_lcc_all_2, graph_snapshots_2= centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "R_values_3, df_lcc_all_3, graph_snapshots_3= centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 4\n",
    "R_values_4, df_lcc_all_4, graph_snapshots_4= centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40%\n",
    "block_size = 1\n",
    "pct = 0.4\n",
    "R_values, df_lcc_all_41, graph_snapshots_41 = centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size, percentage=pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2\n",
    "pct = 0.4\n",
    "R_values, df_lcc_all_42, graph_snapshots_42 = centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size, percentage=pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "pct = 0.4\n",
    "R_values, df_lcc_all_43, graph_snapshots_43 = centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size, percentage=pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 4\n",
    "pct = 0.4\n",
    "R_values, df_lcc_all_44, graph_snapshots_44 = centrality_disruption_analysis(G_Meeting_processed, centr_measures, include_within_LCC=True, block_size=block_size, percentage=pct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
